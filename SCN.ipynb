{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6504606,"sourceType":"datasetVersion","datasetId":3758654},{"sourceId":9523518,"sourceType":"datasetVersion","datasetId":5798855},{"sourceId":9524198,"sourceType":"datasetVersion","datasetId":5799395}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torch.optim as optim\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nimport numpy as np \nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:48:53.480747Z","iopub.execute_input":"2024-10-01T12:48:53.481722Z","iopub.status.idle":"2024-10-01T12:48:53.488417Z","shell.execute_reply.started":"2024-10-01T12:48:53.481666Z","shell.execute_reply":"2024-10-01T12:48:53.487394Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# def initialize_weight_goog(m):\n#     if isinstance(m, nn.Linear):\n#         fan_out = m.weight.size(0)\n#         fan_in = 0\n#         init_range = 1.0 / math.sqrt(fan_in + fan_out)\n#         m.weight.data.uniform_(-init_range, init_range)\n#         m.bias.data.zero_(0)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:48:53.982742Z","iopub.execute_input":"2024-10-01T12:48:53.983629Z","iopub.status.idle":"2024-10-01T12:48:53.987650Z","shell.execute_reply.started":"2024-10-01T12:48:53.983586Z","shell.execute_reply":"2024-10-01T12:48:53.986640Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:48:54.730821Z","iopub.execute_input":"2024-10-01T12:48:54.731496Z","iopub.status.idle":"2024-10-01T12:48:54.736123Z","shell.execute_reply.started":"2024-10-01T12:48:54.731460Z","shell.execute_reply":"2024-10-01T12:48:54.735240Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"class ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:48:55.249704Z","iopub.execute_input":"2024-10-01T12:48:55.250558Z","iopub.status.idle":"2024-10-01T12:48:55.257636Z","shell.execute_reply.started":"2024-10-01T12:48:55.250517Z","shell.execute_reply":"2024-10-01T12:48:55.256628Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"class SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:48:55.733995Z","iopub.execute_input":"2024-10-01T12:48:55.734733Z","iopub.status.idle":"2024-10-01T12:48:55.741625Z","shell.execute_reply.started":"2024-10-01T12:48:55.734696Z","shell.execute_reply":"2024-10-01T12:48:55.740710Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.ca = ChannelAttention(planes)\n        self.sa = SpatialAttention()\n\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out = self.ca(out) * out\n        out = self.sa(out) * out\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:48:56.296902Z","iopub.execute_input":"2024-10-01T12:48:56.297240Z","iopub.status.idle":"2024-10-01T12:48:56.307601Z","shell.execute_reply.started":"2024-10-01T12:48:56.297202Z","shell.execute_reply":"2024-10-01T12:48:56.306636Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"class ResNetWithSCN(nn.Module):\n\n    def __init__(self, block, layers, num_heads, num_classes=7):\n        self.inplanes = 64\n        # self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        super(ResNetWithSCN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        # self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n        self.attention = nn.MultiheadAttention(256, num_heads)\n        self.layer_norm = nn.LayerNorm(256)\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.linear1 = nn.Linear(256,128)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)   \n        self.classifier = nn.Linear(128, num_classes)\n            # nn.Linear(256,128),\n            # nn.ReLU(),\n            # nn.Dropout(0.5),\n            # nn.Linear(128, num_classes)\n        \n        self.alpha = nn.Sequential(\n            # nn.Linear(256,128),\n            # nn.ReLU(),\n            # nn.Dropout(0.5),\n            # nn.Linear(128, num_classes)\n            nn.Linear(128,1),\n            nn.Sigmoid()\n        )\n\n        for m in self.classifier.modules():\n            if isinstance(m, nn.Linear):\n                fan_out = m.weight.size(0)  # fan-out\n                fan_in = m.weight.size(1) if m.weight.dim() > 1 else 0  # fan-in\n                init_range = 1.0 / math.sqrt(fan_in + fan_out)\n                m.weight.data.uniform_(-init_range, init_range)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n        for m in self.alpha.modules():\n            if isinstance(m, nn.Linear):\n                fan_out = m.weight.size(0)  # fan-out\n                fan_in = m.weight.size(1) if m.weight.dim() > 1 else 0  # fan-in\n                init_range = 1.0 / math.sqrt(fan_in + fan_out)\n                m.weight.data.uniform_(-init_range, init_range)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        # x = self.layer4(x)\n\n        # x = self.avgpool(x)\n        # x = x.view(x.size(0), -1)\n        # x = self.fc(x)\n        batch_size, channels, height, width = x.shape\n        x = x.view(batch_size, channels, height*width).permute(0,2,1)\n\n        query = x\n        key = x\n        value = x\n\n        attn_output, _ = self.attention(query, key, value)\n        x = self.layer_norm(attn_output + x)\n        x = x.permute(0,2,1).view(batch_size, channels, height, width)\n        # x = nn.functional.adaptive_avg_pool2d(x,1).view(batch_size, -1)\n        x = self.global_avg_pool(x).view(batch_size, -1)\n        x = self.linear1(x)\n        x = self.relu1(x)\n        x = self.dropout1(x)\n        \n        attention_weights = self.alpha(x)\n        out = attention_weights * self.classifier(x)\n\n        return attention_weights, out","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:48:56.911089Z","iopub.execute_input":"2024-10-01T12:48:56.911422Z","iopub.status.idle":"2024-10-01T12:48:56.933658Z","shell.execute_reply.started":"2024-10-01T12:48:56.911389Z","shell.execute_reply":"2024-10-01T12:48:56.932892Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def resnet18_cbam_with_SCN(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    num_heads = 4\n    model = ResNetWithSCN(BasicBlock, [2, 2, 2, 2], num_heads)\n    # if pretrained:\n    #     pretrained_state_dict = model_zoo.load_url(model_urls['resnet18'])\n    #     now_state_dict        = model.state_dict()\n    #     now_state_dict.update(pretrained_state_dict)\n    #     model.load_state_dict(now_state_dict, strict=False)\n    \n    # New Code Starts From Here\n    if pretrained:\n        checkpoint = torch.load(r\"/kaggle/input/model-pretrained-weights/tot20epoch_4heads_epoch_13_loss_0.07219641906097725.pth\")\n        model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n    # Ends Here\n\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:49:08.104718Z","iopub.execute_input":"2024-10-01T12:49:08.105117Z","iopub.status.idle":"2024-10-01T12:49:08.111595Z","shell.execute_reply.started":"2024-10-01T12:49:08.105078Z","shell.execute_reply":"2024-10-01T12:49:08.110570Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:49:08.870891Z","iopub.execute_input":"2024-10-01T12:49:08.871529Z","iopub.status.idle":"2024-10-01T12:49:08.878394Z","shell.execute_reply.started":"2024-10-01T12:49:08.871490Z","shell.execute_reply":"2024-10-01T12:49:08.877228Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# model = resnet18_cbam_with_SCN()\n# model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:49:09.673408Z","iopub.execute_input":"2024-10-01T12:49:09.673773Z","iopub.status.idle":"2024-10-01T12:49:09.678034Z","shell.execute_reply.started":"2024-10-01T12:49:09.673737Z","shell.execute_reply":"2024-10-01T12:49:09.676902Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, image_dir, csv_file, transform=None):\n        self.image_dir = image_dir\n        self.transform = transform\n\n        # Read the CSV file\n        self.data_frame = pd.read_csv(csv_file)\n\n        # Ensure the CSV file has columns 'filename' and 'class'\n        assert 'image' in self.data_frame.columns\n        assert 'label' in self.data_frame.columns\n        \n        self.labels = self.data_frame['label'].values - 1\n\n    def __len__(self):\n        return len(self.data_frame)\n\n    def __getitem__(self, idx):\n        # Get the filename and class label\n        img_name = self.data_frame.iloc[idx, 0]  # Get the filename from the CSV\n        class_label = self.labels[idx]  # Get the class label from the CSV\n\n        # Construct the path to the image based on its class label\n        class_folder = os.path.join(self.image_dir, str(class_label+1))  # Convert class label to string\n        img_path = os.path.join(class_folder, img_name)\n\n        # Load the image\n        image = Image.open(img_path).convert('RGB')\n\n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n\n        return image, class_label, idx","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:49:10.494949Z","iopub.execute_input":"2024-10-01T12:49:10.495338Z","iopub.status.idle":"2024-10-01T12:49:10.507480Z","shell.execute_reply.started":"2024-10-01T12:49:10.495302Z","shell.execute_reply":"2024-10-01T12:49:10.506256Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"os.getcwd()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:49:11.345902Z","iopub.execute_input":"2024-10-01T12:49:11.346285Z","iopub.status.idle":"2024-10-01T12:49:11.353492Z","shell.execute_reply.started":"2024-10-01T12:49:11.346249Z","shell.execute_reply":"2024-10-01T12:49:11.352384Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"os.makedirs('models', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:49:13.711829Z","iopub.execute_input":"2024-10-01T12:49:13.712663Z","iopub.status.idle":"2024-10-01T12:49:13.717357Z","shell.execute_reply.started":"2024-10-01T12:49:13.712623Z","shell.execute_reply":"2024-10-01T12:49:13.716203Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"def run_training():\n\n    # args = parse_args()\n    # imagenet_pretrained = True\n    # res18 = ResNetWithSCN(pretrained = imagenet_pretrained, drop_rate = args.drop_rate)\n    model = resnet18_cbam_with_SCN()\n    model.to(device)\n    \n    # if not imagenet_pretrained:\n    #      for m in res18.modules():\n    #         initialize_weight_goog(m)\n            \n    # if args.pretrained:\n    #     print(\"Loading pretrained weights...\", args.pretrained) \n    #     pretrained = torch.load(args.pretrained)\n    #     pretrained_state_dict = pretrained['state_dict']\n    #     model_state_dict = res18.state_dict()\n    #     loaded_keys = 0\n    #     total_keys = 0\n    #     for key in pretrained_state_dict:\n    #         if  ((key=='module.fc.weight')|(key=='module.fc.bias')):\n    #             pass\n    #         else:    \n    #             model_state_dict[key] = pretrained_state_dict[key]\n    #             total_keys+=1\n    #             if key in model_state_dict:\n    #                 loaded_keys+=1\n    #     print(\"Loaded params num:\", loaded_keys)\n    #     print(\"Total params num:\", total_keys)\n    #     res18.load_state_dict(model_state_dict, strict = False)  \n        \n    # data_transforms = transforms.Compose([\n    #     transforms.ToPILImage(),\n    #     transforms.Resize((224, 224)),\n    #     transforms.ToTensor(),\n    #     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n    #                              std=[0.229, 0.224, 0.225]),\n    #     transforms.RandomErasing(scale=(0.02,0.25))])\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n    ])\n    \n    # train_dataset = RafDataSet(args.raf_path, phase = 'train', transform = data_transforms, basic_aug = True)  \n    image_directory = r\"/kaggle/input/raf-db-dataset/DATASET/train\"  # Directory containing class subfolders\n    csv_file_path = r\"/kaggle/input/raf-db-dataset/train_labels.csv\"\n    train_dataset = CustomImageDataset(image_dir=image_directory, csv_file=csv_file_path, transform=transform)\n    train_loader = DataLoader(train_dataset, batch_size=32, num_workers=4, pin_memory=True, shuffle=True)\n    \n    # print('Train set size:', train_dataset.__len__())\n    # train_loader = torch.utils.data.DataLoader(train_dataset,\n    #                                            batch_size = args.batch_size,\n    #                                            num_workers = args.workers,\n    #                                            shuffle = True,  \n    #                                            pin_memory = True)\n    test_image_directory = r\"/kaggle/input/raf-db-dataset/DATASET/test\"\n    test_csv_file_path = r\"/kaggle/input/raf-db-dataset/test_labels.csv\"\n    test_dataset = CustomImageDataset(test_image_directory, test_csv_file_path, transform)\n    test_loader = DataLoader(test_dataset, batch_size=32,shuffle=False)\n    # data_transforms_val = transforms.Compose([\n    #     transforms.ToPILImage(),\n    #     transforms.Resize((224, 224)),\n    #     transforms.ToTensor(),\n    #     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n    #                              std=[0.229, 0.224, 0.225])])                                           \n    # val_dataset = RafDataSet(args.raf_path, phase = 'test', transform = data_transforms_val)    \n    # print('Validation set size:', val_dataset.__len__())\n    \n    # val_loader = torch.utils.data.DataLoader(val_dataset,\n    #                                            batch_size = args.batch_size,\n    #                                            num_workers = args.workers,\n    #                                            shuffle = False,  \n    #                                            pin_memory = True)\n    \n    params = model.parameters()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n    \n    # if args.optimizer == 'adam':\n    #     optimizer = torch.optim.Adam(params,weight_decay = 1e-4)\n    # elif args.optimizer == 'sgd':\n    #     optimizer = torch.optim.SGD(params, args.lr,\n    #                                 momentum=args.momentum,\n    #                                 weight_decay = 1e-4)\n    # else:\n    #     raise ValueError(\"Optimizer not supported.\")\n    \n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n    # res18 = res18.cuda()\n    # criterion = torch.nn.CrossEntropyLoss()\n    \n    margin_1 = 0.15\n    margin_2 = 0.20\n    beta = 0.7\n    \n    for i in range(1, 20 + 1):\n        running_loss = 0.0\n        correct_sum = 0\n        iter_cnt = 0\n        model.train()\n        # for batch_i, (imgs, targets, indexes) in enumerate(train_loader):\n        for images, labels, indexes in tqdm(train_loader):\n            batch_sz = images.size(0) \n            iter_cnt += 1\n            tops = int(batch_sz* beta)\n            optimizer.zero_grad()\n            # imgs = imgs.cuda()\n            images = images.to(device) \n            labels = labels.to(device)\n            attention_weights, outputs = model(images)\n            \n            # Rank Regularization\n            _, top_idx = torch.topk(attention_weights.squeeze(), tops)\n            _, down_idx = torch.topk(attention_weights.squeeze(), batch_sz - tops, largest = False)\n\n            high_group = attention_weights[top_idx]\n            low_group = attention_weights[down_idx]\n            high_mean = torch.mean(high_group)\n            low_mean = torch.mean(low_group)\n            # diff  = margin_1 - (high_mean - low_mean)\n            diff  = low_mean - high_mean + margin_1\n\n            if diff > 0:\n                RR_loss = diff\n            else:\n                RR_loss = 0.0\n            \n            # targets = targets.cuda()\n            loss = criterion(outputs, labels) + RR_loss \n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss\n            _, predicts = torch.max(outputs, 1)\n            correct_num = torch.eq(predicts, labels).sum()\n            correct_sum += correct_num\n\n            # Relabel samples\n            if i >= 10:\n                sm = torch.softmax(outputs, dim = 1)\n                Pmax, predicted_labels = torch.max(sm, 1) # predictions\n                Pgt = torch.gather(sm, 1, labels.view(-1,1)).squeeze() # retrieve predicted probabilities of targets\n                true_or_false = Pmax - Pgt > margin_2\n                update_idx = true_or_false.nonzero().squeeze() # get samples' index in this mini-batch where (Pmax - Pgt > margin_2)\n                label_idx = indexes[update_idx.cpu()] # get samples' index in train_loader\n                relabels = predicted_labels[update_idx] # predictions where (Pmax - Pgt > margin_2)\n                train_loader.dataset.labels[label_idx.cpu().numpy()] = relabels.cpu().numpy() # relabel samples in train_loader\n                \n        scheduler.step()\n        acc = correct_sum.float() / float(train_dataset.__len__())\n        running_loss = running_loss/iter_cnt\n        print('[Epoch %d] Training accuracy: %.4f. Loss: %.3f' % (i, acc, running_loss))\n        \n        with torch.no_grad():\n            running_loss = 0.0\n            iter_cnt = 0\n            bingo_cnt = 0\n            sample_cnt = 0\n            model.eval()\n            # for batch_i, (imgs, targets, _) in enumerate(val_loader):\n            for imgs, targets, _ in tqdm(test_loader):\n                _, outputs = model(imgs.cuda())\n                targets = targets.cuda()\n                loss = criterion(outputs, targets)\n                running_loss += loss\n                iter_cnt+=1\n                _, predicts = torch.max(outputs, 1)\n                correct_num  = torch.eq(predicts,targets)\n                bingo_cnt += correct_num.sum().cpu()\n                sample_cnt += outputs.size(0)\n                \n            running_loss = running_loss/iter_cnt   \n            acc = bingo_cnt.float()/float(sample_cnt)\n            acc = np.around(acc.numpy(),4)\n            print(\"[Epoch %d] Test accuracy:%.4f. Loss:%.3f\" % (i, acc, running_loss))\n           \n            if acc > 0.76 :\n                torch.save({'iter': i,\n                            'model_state_dict': model.state_dict(),\n                             'optimizer_state_dict': optimizer.state_dict(),},\n                            os.path.join('models', \"SCN1_epoch_\"+str(i)+\"_acc_\"+str(acc)+\".pth\"))\n                print('Model saved.')","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:49:31.908234Z","iopub.execute_input":"2024-10-01T12:49:31.908607Z","iopub.status.idle":"2024-10-01T12:49:31.936477Z","shell.execute_reply.started":"2024-10-01T12:49:31.908569Z","shell.execute_reply":"2024-10-01T12:49:31.935389Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"run_training()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:49:34.118764Z","iopub.execute_input":"2024-10-01T12:49:34.119774Z","iopub.status.idle":"2024-10-01T12:57:58.305280Z","shell.execute_reply.started":"2024-10-01T12:49:34.119722Z","shell.execute_reply":"2024-10-01T12:57:58.303385Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3631545526.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(r\"/kaggle/input/model-pretrained-weights/tot20epoch_4heads_epoch_13_loss_0.07219641906097725.pth\")\n100%|██████████| 384/384 [00:38<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Training accuracy: 0.9501. Loss: 0.194\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:13<00:00,  6.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Test accuracy:0.7419. Loss:1.062\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 384/384 [00:38<00:00,  9.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Training accuracy: 0.9773. Loss: 0.078\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:11<00:00,  8.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Test accuracy:0.7096. Loss:1.174\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 384/384 [00:38<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Training accuracy: 0.9846. Loss: 0.053\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:11<00:00,  8.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Test accuracy:0.7262. Loss:1.381\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 384/384 [00:38<00:00,  9.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Training accuracy: 0.9838. Loss: 0.054\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:11<00:00,  8.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Test accuracy:0.7484. Loss:1.156\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 384/384 [00:38<00:00,  9.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Training accuracy: 0.9897. Loss: 0.036\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:11<00:00,  8.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Test accuracy:0.7073. Loss:1.502\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 384/384 [00:38<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Training accuracy: 0.9937. Loss: 0.020\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:11<00:00,  8.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Test accuracy:0.7040. Loss:1.695\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 384/384 [00:38<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Training accuracy: 0.9932. Loss: 0.025\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:11<00:00,  8.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Test accuracy:0.7467. Loss:1.245\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 384/384 [00:38<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Training accuracy: 0.9942. Loss: 0.023\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:11<00:00,  8.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Test accuracy:0.7184. Loss:1.656\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 384/384 [00:38<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Training accuracy: 0.9976. Loss: 0.008\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:11<00:00,  8.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Test accuracy:0.6864. Loss:2.000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 384/384 [00:39<00:00,  9.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Training accuracy: 0.9963. Loss: 0.013\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 96/96 [00:10<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Test accuracy:0.6457. Loss:2.090\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 2/384 [00:00<01:44,  3.66it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[76], line 103\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# for batch_i, (imgs, targets, indexes) in enumerate(train_loader):\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels, indexes \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m    104\u001b[0m     batch_sz \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \n\u001b[1;32m    105\u001b[0m     iter_cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1324\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1323\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mFileNotFoundError\u001b[0m: Caught FileNotFoundError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_30/151118697.py\", line 28, in __getitem__\n    image = Image.open(img_path).convert('RGB')\n  File \"/opt/conda/lib/python3.10/site-packages/PIL/Image.py\", line 3431, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/input/raf-db-dataset/DATASET/train/4/train_00562_aligned.jpg'\n"],"ename":"FileNotFoundError","evalue":"Caught FileNotFoundError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_30/151118697.py\", line 28, in __getitem__\n    image = Image.open(img_path).convert('RGB')\n  File \"/opt/conda/lib/python3.10/site-packages/PIL/Image.py\", line 3431, in open\n    fp = builtins.open(filename, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/input/raf-db-dataset/DATASET/train/4/train_00562_aligned.jpg'\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
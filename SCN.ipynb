{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdf797c-523f-4eba-9e38-42a8a0c2bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab0ed83-d22d-4239-9939-3aa4f99c89b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_weight_goog(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         fan_out = m.weight.size(0)\n",
    "#         fan_in = 0\n",
    "#         init_range = 1.0 / math.sqrt(fan_in + fan_out)\n",
    "#         m.weight.data.uniform_(-init_range, init_range)\n",
    "#         m.bias.data.zero_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fd7c689-5ad0-40d7-a0b1-214e6b3c5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetWithSCN(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_heads, num_classes=7):\n",
    "        self.inplanes = 64\n",
    "        # self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        # self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.attention = nn.MultiheadAttention(256, num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(256)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear1 = nn.Linear(256,128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)   \n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "            # nn.Linear(256,128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(128, num_classes)\n",
    "        \n",
    "        self.alpha = nn.Sequential(\n",
    "            # nn.Linear(256,128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(128, num_classes)\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                fan_out = m.weight.size(0)  # fan-out\n",
    "                fan_in = m.weight.size(1) if m.weight.dim() > 1 else 0  # fan-in\n",
    "                init_range = 1.0 / math.sqrt(fan_in + fan_out)\n",
    "                m.weight.data.uniform_(-init_range, init_range)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "        for m in self.alpha.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                fan_out = m.weight.size(0)  # fan-out\n",
    "                fan_in = m.weight.size(1) if m.weight.dim() > 1 else 0  # fan-in\n",
    "                init_range = 1.0 / math.sqrt(fan_in + fan_out)\n",
    "                m.weight.data.uniform_(-init_range, init_range)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "\n",
    "        # x = self.avgpool(x)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # x = self.fc(x)\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.view(batch_size, channels, height*width).permute(0,2,1)\n",
    "\n",
    "        query = x\n",
    "        key = x\n",
    "        value = x\n",
    "\n",
    "        attn_output, _ = self.attention(query, key, value)\n",
    "        x = self.layer_norm(attn_output + x)\n",
    "        x = x.permute(0,2,1).view(batch_size, channels, height, width)\n",
    "        # x = nn.functional.adaptive_avg_pool2d(x,1).view(batch_size, -1)\n",
    "        x = self.global_avg_pool(x).view(batch_size, -1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        attention_weights = self.alpha(x)\n",
    "        out = attention_weights * self.classifier(x)\n",
    "\n",
    "        return attention_weights, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b693f86-dfa4-430e-b637-1f038266cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18_cbam_with_SCN(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    num_heads = 4\n",
    "    model = ResNetWithSCN(BasicBlock, [2, 2, 2, 2], num_heads)\n",
    "    # if pretrained:\n",
    "    #     pretrained_state_dict = model_zoo.load_url(model_urls['resnet18'])\n",
    "    #     now_state_dict        = model.state_dict()\n",
    "    #     now_state_dict.update(pretrained_state_dict)\n",
    "    #     model.load_state_dict(now_state_dict, strict=False)\n",
    "    \n",
    "    # New Code Starts From Here\n",
    "    if pretrained:\n",
    "        checkpoint = torch.load(r\"/kaggle/input/epoch-4-weights/epoch_4_loss_0.4130449585500173.pth\")\n",
    "        model.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    # Ends Here\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac04fef-aaa6-4e6f-b2a2-9ec616b18213",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac83a9-ae3e-4367-af0f-ac36605f5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resnet18_cbam_with_SCN()\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d11f0-c281-4e4c-9abb-cb29cfdd5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, csv_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Read the CSV file\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "\n",
    "        # Ensure the CSV file has columns 'filename' and 'class'\n",
    "        assert 'image' in self.data_frame.columns\n",
    "        assert 'label' in self.data_frame.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the filename and class label\n",
    "        img_name = self.data_frame.iloc[idx, 0]  # Get the filename from the CSV\n",
    "        class_label = self.data_frame.iloc[idx, 1] - 1  # Get the class label from the CSV\n",
    "\n",
    "        # Construct the path to the image based on its class label\n",
    "        class_folder = os.path.join(self.image_dir, str(class_label+1))  # Convert class label to string\n",
    "        img_path = os.path.join(class_folder, img_name)\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, class_label, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43904297-d2c4-468e-8eba-a2ef85df62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5ac8a8-02ff-4b85-9a83-353e094981c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "\n",
    "    # args = parse_args()\n",
    "    # imagenet_pretrained = True\n",
    "    # res18 = ResNetWithSCN(pretrained = imagenet_pretrained, drop_rate = args.drop_rate)\n",
    "    model = resnet18_cbam_with_SCN()\n",
    "    model.to(device)\n",
    "    \n",
    "    # if not imagenet_pretrained:\n",
    "    #      for m in res18.modules():\n",
    "    #         initialize_weight_goog(m)\n",
    "            \n",
    "    # if args.pretrained:\n",
    "    #     print(\"Loading pretrained weights...\", args.pretrained) \n",
    "    #     pretrained = torch.load(args.pretrained)\n",
    "    #     pretrained_state_dict = pretrained['state_dict']\n",
    "    #     model_state_dict = res18.state_dict()\n",
    "    #     loaded_keys = 0\n",
    "    #     total_keys = 0\n",
    "    #     for key in pretrained_state_dict:\n",
    "    #         if  ((key=='module.fc.weight')|(key=='module.fc.bias')):\n",
    "    #             pass\n",
    "    #         else:    \n",
    "    #             model_state_dict[key] = pretrained_state_dict[key]\n",
    "    #             total_keys+=1\n",
    "    #             if key in model_state_dict:\n",
    "    #                 loaded_keys+=1\n",
    "    #     print(\"Loaded params num:\", loaded_keys)\n",
    "    #     print(\"Total params num:\", total_keys)\n",
    "    #     res18.load_state_dict(model_state_dict, strict = False)  \n",
    "        \n",
    "    # data_transforms = transforms.Compose([\n",
    "    #     transforms.ToPILImage(),\n",
    "    #     transforms.Resize((224, 224)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                              std=[0.229, 0.224, 0.225]),\n",
    "    #     transforms.RandomErasing(scale=(0.02,0.25))])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # train_dataset = RafDataSet(args.raf_path, phase = 'train', transform = data_transforms, basic_aug = True)  \n",
    "    image_directory = r\"/kaggle/input/raf-db-dataset/DATASET/train\"  # Directory containing class subfolders\n",
    "    csv_file_path = r\"/kaggle/input/raf-db-dataset/train_labels.csv\"\n",
    "    train_dataset = CustomImageDataset(image_dir=image_directory, csv_file=csv_file_path, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, num_workers=4, pin_memory=True, shuffle=True)\n",
    "    \n",
    "    # print('Train set size:', train_dataset.__len__())\n",
    "    # train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "    #                                            batch_size = args.batch_size,\n",
    "    #                                            num_workers = args.workers,\n",
    "    #                                            shuffle = True,  \n",
    "    #                                            pin_memory = True)\n",
    "    test_image_directory = r\"/kaggle/input/raf-db-dataset/DATASET/test\"\n",
    "    test_csv_file_path = r\"/kaggle/input/raf-db-dataset/test_labels.csv\"\n",
    "    test_dataset = CustomImageDataset(test_image_directory, test_csv_file_path, transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32,shuffle=False)\n",
    "    # data_transforms_val = transforms.Compose([\n",
    "    #     transforms.ToPILImage(),\n",
    "    #     transforms.Resize((224, 224)),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                              std=[0.229, 0.224, 0.225])])                                           \n",
    "    # val_dataset = RafDataSet(args.raf_path, phase = 'test', transform = data_transforms_val)    \n",
    "    # print('Validation set size:', val_dataset.__len__())\n",
    "    \n",
    "    # val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "    #                                            batch_size = args.batch_size,\n",
    "    #                                            num_workers = args.workers,\n",
    "    #                                            shuffle = False,  \n",
    "    #                                            pin_memory = True)\n",
    "    \n",
    "    params = model.parameters()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    \n",
    "    # if args.optimizer == 'adam':\n",
    "    #     optimizer = torch.optim.Adam(params,weight_decay = 1e-4)\n",
    "    # elif args.optimizer == 'sgd':\n",
    "    #     optimizer = torch.optim.SGD(params, args.lr,\n",
    "    #                                 momentum=args.momentum,\n",
    "    #                                 weight_decay = 1e-4)\n",
    "    # else:\n",
    "    #     raise ValueError(\"Optimizer not supported.\")\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n",
    "    # res18 = res18.cuda()\n",
    "    # criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    margin_1 = 0.15\n",
    "    margin_2 = 0.20\n",
    "    beta = 0.7\n",
    "    \n",
    "    for i in range(1, 20 + 1):\n",
    "        running_loss = 0.0\n",
    "        correct_sum = 0\n",
    "        iter_cnt = 0\n",
    "        model.train()\n",
    "        # for batch_i, (imgs, targets, indexes) in enumerate(train_loader):\n",
    "        for images, labels, indexes in tqdm(train_loader):\n",
    "            batch_sz = images.size(0) \n",
    "            iter_cnt += 1\n",
    "            tops = int(batch_sz* beta)\n",
    "            optimizer.zero_grad()\n",
    "            # imgs = imgs.cuda()\n",
    "            images = images.to(device) \n",
    "            labels = labels.to(device)\n",
    "            attention_weights, outputs = model(imgs)\n",
    "            \n",
    "            # Rank Regularization\n",
    "            _, top_idx = torch.topk(attention_weights.squeeze(), tops)\n",
    "            _, down_idx = torch.topk(attention_weights.squeeze(), batch_sz - tops, largest = False)\n",
    "\n",
    "            high_group = attention_weights[top_idx]\n",
    "            low_group = attention_weights[down_idx]\n",
    "            high_mean = torch.mean(high_group)\n",
    "            low_mean = torch.mean(low_group)\n",
    "            # diff  = margin_1 - (high_mean - low_mean)\n",
    "            diff  = low_mean - high_mean + margin_1\n",
    "\n",
    "            if diff > 0:\n",
    "                RR_loss = diff\n",
    "            else:\n",
    "                RR_loss = 0.0\n",
    "            \n",
    "            # targets = targets.cuda()\n",
    "            loss = criterion(outputs, labels) + RR_loss \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss\n",
    "            _, predicts = torch.max(outputs, 1)\n",
    "            correct_num = torch.eq(predicts, targets).sum()\n",
    "            correct_sum += correct_num\n",
    "\n",
    "            # Relabel samples\n",
    "            if i >= 10:\n",
    "                sm = torch.softmax(outputs, dim = 1)\n",
    "                Pmax, predicted_labels = torch.max(sm, 1) # predictions\n",
    "                Pgt = torch.gather(sm, 1, targets.view(-1,1)).squeeze() # retrieve predicted probabilities of targets\n",
    "                true_or_false = Pmax - Pgt > margin_2\n",
    "                update_idx = true_or_false.nonzero().squeeze() # get samples' index in this mini-batch where (Pmax - Pgt > margin_2)\n",
    "                label_idx = indexes[update_idx] # get samples' index in train_loader\n",
    "                relabels = predicted_labels[update_idx] # predictions where (Pmax - Pgt > margin_2)\n",
    "                train_loader.dataset.label[label_idx.cpu().numpy()] = relabels.cpu().numpy() # relabel samples in train_loader\n",
    "                \n",
    "        scheduler.step()\n",
    "        acc = correct_sum.float() / float(train_dataset.__len__())\n",
    "        running_loss = running_loss/iter_cnt\n",
    "        print('[Epoch %d] Training accuracy: %.4f. Loss: %.3f' % (i, acc, running_loss))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            iter_cnt = 0\n",
    "            bingo_cnt = 0\n",
    "            sample_cnt = 0\n",
    "            model.eval()\n",
    "            # for batch_i, (imgs, targets, _) in enumerate(val_loader):\n",
    "            for imgs, targets, _ in tqdm(test_loader):\n",
    "                _, outputs = res18(imgs.cuda())\n",
    "                targets = targets.cuda()\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_loss += loss\n",
    "                iter_cnt+=1\n",
    "                _, predicts = torch.max(outputs, 1)\n",
    "                correct_num  = torch.eq(predicts,targets)\n",
    "                bingo_cnt += correct_num.sum().cpu()\n",
    "                sample_cnt += outputs.size(0)\n",
    "                \n",
    "            running_loss = running_loss/iter_cnt   \n",
    "            acc = bingo_cnt.float()/float(sample_cnt)\n",
    "            acc = np.around(acc.numpy(),4)\n",
    "            print(\"[Epoch %d] Test accuracy:%.4f. Loss:%.3f\" % (i, acc, running_loss))\n",
    "           \n",
    "            if acc > 0.76 :\n",
    "                torch.save({'iter': i,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                             'optimizer_state_dict': optimizer.state_dict(),},\n",
    "                            os.path.join('models', \"SCN_epoch_\"+str(i)+\"_acc_\"+str(acc)+\".pth\"))\n",
    "                print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee4b64-5938-4246-bb8c-44cf5cfa204b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b32ffe-9edc-413f-80be-5f9eef723374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d1dd0-6fe6-46c0-9f61-3df2218903a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf1a6f-e963-4844-9550-06d1187fffba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755b652-e0fd-4ee4-bee6-92330a939732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630780da-a23c-45e3-a994-e0c313f39041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d840a-b950-4c39-95d1-8e89571b5b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33358d10-d454-4759-9460-7da2546b0968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98cb422-5214-4568-bda7-f7a0cceb8987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8a8d1c-f9f0-4933-9892-770754c49cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa071d-a38b-48f5-86b6-502966d12823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

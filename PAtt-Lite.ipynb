{"metadata":{"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6504606,"sourceType":"datasetVersion","datasetId":3758654}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport torch.optim as optim\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nimport numpy as np \nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torchvision.transforms as T\n# from pytorchcv.model_provider import get_model","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:48.594519Z","iopub.execute_input":"2024-10-04T06:06:48.595379Z","iopub.status.idle":"2024-10-04T06:06:48.603627Z","shell.execute_reply.started":"2024-10-04T06:06:48.595337Z","shell.execute_reply":"2024-10-04T06:06:48.602615Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"# pip install pytorchcv","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:48.712943Z","iopub.execute_input":"2024-10-04T06:06:48.713962Z","iopub.status.idle":"2024-10-04T06:06:48.718341Z","shell.execute_reply.started":"2024-10-04T06:06:48.713918Z","shell.execute_reply":"2024-10-04T06:06:48.717264Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:48.720878Z","iopub.execute_input":"2024-10-04T06:06:48.721493Z","iopub.status.idle":"2024-10-04T06:06:48.728151Z","shell.execute_reply.started":"2024-10-04T06:06:48.721435Z","shell.execute_reply":"2024-10-04T06:06:48.727363Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"class PatchExtraction(nn.Module):\n    def __init__(self):\n        super(PatchExtraction, self).__init__()\n        # First separable convolution (depthwise + pointwise)\n        self.depthwise_conv1 = nn.Conv2d(320, 320, kernel_size=4, stride=4, padding=1, groups=320)\n        self.pointwise_conv1 = nn.Conv2d(320, 256, kernel_size=1, stride=1, padding=0)\n        \n        # Second separable convolution (depthwise + pointwise)\n        self.depthwise_conv2 = nn.Conv2d(256, 256, kernel_size=2, stride=2, padding=0, groups=256)\n        self.pointwise_conv2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n\n        # Normal Conv (used directly)\n        self.conv3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        # First separable convolution\n        x = F.relu(self.depthwise_conv1(x))\n        x = F.relu(self.pointwise_conv1(x))\n\n        # Second separable convolution\n        x = F.relu(self.depthwise_conv2(x))\n        x = F.relu(self.pointwise_conv2(x))\n\n        # Normal convolution\n        x = F.relu(self.conv3(x))\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:48.729534Z","iopub.execute_input":"2024-10-04T06:06:48.729935Z","iopub.status.idle":"2024-10-04T06:06:48.740342Z","shell.execute_reply.started":"2024-10-04T06:06:48.729884Z","shell.execute_reply":"2024-10-04T06:06:48.739270Z"},"trusted":true},"execution_count":232,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, embed_size, num_heads=1):\n        super(SelfAttention, self).__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads)\n\n    def forward(self, x):\n        # Attention expects input of shape [sequence_length, batch_size, embed_dim]\n        x = x.unsqueeze(0)  # Adding sequence length as 1\n        attn_output, _ = self.attention(x, x, x)\n        return attn_output.squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:48.741809Z","iopub.execute_input":"2024-10-04T06:06:48.742385Z","iopub.status.idle":"2024-10-04T06:06:48.752031Z","shell.execute_reply.started":"2024-10-04T06:06:48.742335Z","shell.execute_reply":"2024-10-04T06:06:48.751240Z"},"trusted":true},"execution_count":233,"outputs":[]},{"cell_type":"code","source":"# directory_path = '/kaggle/working'\n\n# # Method 1: Using os to remove files\n# for filename in os.listdir(directory_path):\n#     file_path = os.path.join(directory_path, filename)\n#     try:\n#         if os.path.isfile(file_path):\n#             os.unlink(file_path)  # Remove the file\n#           # Remove the directory and all its contents\n#     except Exception as e:\n#         print(f'Failed to delete {file_path}. Reason: {e}')","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:40:30.393145Z","iopub.execute_input":"2024-10-04T06:40:30.394080Z","iopub.status.idle":"2024-10-04T06:40:30.400166Z","shell.execute_reply.started":"2024-10-04T06:40:30.394013Z","shell.execute_reply":"2024-10-04T06:40:30.398904Z"},"trusted":true},"execution_count":258,"outputs":[]},{"cell_type":"code","source":"dummy_input = torch.randn(1, 3, 224, 224)\nx = dummy_input\nfor layer in backbone.features:\n    x = layer(x)  # Pass the input through the layer\n    print(f\"{layer.__class__.__name__}: {x.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:38:00.507966Z","iopub.execute_input":"2024-10-04T06:38:00.509073Z","iopub.status.idle":"2024-10-04T06:38:00.592813Z","shell.execute_reply.started":"2024-10-04T06:38:00.509014Z","shell.execute_reply":"2024-10-04T06:38:00.591741Z"},"trusted":true},"execution_count":256,"outputs":[{"name":"stdout","text":"Conv2dNormActivation: torch.Size([1, 32, 112, 112])\nInvertedResidual: torch.Size([1, 16, 112, 112])\nInvertedResidual: torch.Size([1, 24, 56, 56])\nInvertedResidual: torch.Size([1, 24, 56, 56])\nInvertedResidual: torch.Size([1, 32, 28, 28])\nInvertedResidual: torch.Size([1, 32, 28, 28])\nInvertedResidual: torch.Size([1, 32, 28, 28])\nInvertedResidual: torch.Size([1, 64, 14, 14])\nInvertedResidual: torch.Size([1, 64, 14, 14])\nInvertedResidual: torch.Size([1, 64, 14, 14])\nInvertedResidual: torch.Size([1, 64, 14, 14])\nInvertedResidual: torch.Size([1, 96, 14, 14])\nInvertedResidual: torch.Size([1, 96, 14, 14])\nInvertedResidual: torch.Size([1, 96, 14, 14])\nInvertedResidual: torch.Size([1, 160, 7, 7])\nInvertedResidual: torch.Size([1, 160, 7, 7])\nInvertedResidual: torch.Size([1, 160, 7, 7])\nInvertedResidual: torch.Size([1, 320, 7, 7])\nConv2dNormActivation: torch.Size([1, 1280, 7, 7])\n","output_type":"stream"}]},{"cell_type":"code","source":"# backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n\n# # Keep only the first layer\n# first_layer = nn.Sequential(*list(backbone.features.children())[:-1])\n\n# # Create a dummy input tensor\n# dummy_input = torch.randn(1, 3, 224, 224)  # Batch size of 1, 3 channels, 224x224 size\n\n# # Pass the dummy input through the first layer\n# output = first_layer(dummy_input)\n\n# # Print the output size\n# print(\"Output size:\", output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:41:53.517974Z","iopub.execute_input":"2024-10-04T06:41:53.518972Z","iopub.status.idle":"2024-10-04T06:41:53.733073Z","shell.execute_reply.started":"2024-10-04T06:41:53.518925Z","shell.execute_reply":"2024-10-04T06:41:53.731995Z"},"trusted":true},"execution_count":259,"outputs":[{"name":"stdout","text":"Output size: torch.Size([1, 320, 7, 7])\n","output_type":"stream"}]},{"cell_type":"code","source":"class PattLite(nn.Module):\n    def __init__(self):\n        super(PattLite, self).__init__()\n        \n        # Preprocessing: resizing and augmentation\n#         self.transform = transforms.Compose([\n#             transforms.Resize((224, 224)),\n\n#             transforms.ToTensor(),\n#         ])\n        \n        # Backbone (MobileNet with last 29 layers removed)\n        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n        self.backbone = nn.Sequential(*list(self.backbone.features.children())[:-1])\n#         for param in self.backbone.parameters():\n#             param.requires_grad = False\n\n        # Patch extraction, attention, and global average pooling layers\n        self.patch_extraction = PatchExtraction()\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n#         self.dropout = nn.Dropout(0.1)\n\n        # Pre-classification layer\n        self.pre_classification = nn.Sequential(\n            nn.Linear(256, 32),\n            nn.ReLU(),\n            nn.BatchNorm1d(32)\n        )\n\n        # Self-attention\n        self.self_attention = SelfAttention(embed_size=32)\n\n        # Final classification layer\n        self.classifier = nn.Linear(32, 7)\n\n    def forward(self, x):\n        # Apply transformations\n#         x = self.transform(x)\n        \n        # Backbone (MobileNetV2)\n        x = self.backbone(x)\n        \n        # Patch extraction\n        x = self.patch_extraction(x)\n        \n        # Global average pooling\n        x = self.global_avg_pool(x)\n        x = x.view(x.size(0), -1)  # Flatten (N, 256)\n        \n        # Dropout before final classification\n#         x = self.dropout(x)\n        \n        # Pre-classification layer\n        x = self.pre_classification(x)\n        \n        # Self-attention (expects [seq_len, batch_size, embed_dim])\n#         x = self.self_attention(x.unsqueeze(0)).squeeze(0)\n        x = self.self_attention(x).squeeze(0)# Apply self-attention\n        \n        # Final classification layer\n        x = self.classifier(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:48.772422Z","iopub.execute_input":"2024-10-04T06:06:48.772750Z","iopub.status.idle":"2024-10-04T06:06:48.785614Z","shell.execute_reply.started":"2024-10-04T06:06:48.772711Z","shell.execute_reply":"2024-10-04T06:06:48.784718Z"},"trusted":true},"execution_count":236,"outputs":[]},{"cell_type":"code","source":"# pip install --upgrade torchvision","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-04T06:06:48.833337Z","iopub.execute_input":"2024-10-04T06:06:48.834339Z","iopub.status.idle":"2024-10-04T06:06:48.838574Z","shell.execute_reply.started":"2024-10-04T06:06:48.834280Z","shell.execute_reply":"2024-10-04T06:06:48.837675Z"},"trusted":true},"execution_count":237,"outputs":[]},{"cell_type":"code","source":"model = PattLite()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:48.840497Z","iopub.execute_input":"2024-10-04T06:06:48.840813Z","iopub.status.idle":"2024-10-04T06:06:48.982906Z","shell.execute_reply.started":"2024-10-04T06:06:48.840780Z","shell.execute_reply":"2024-10-04T06:06:48.981953Z"},"trusted":true},"execution_count":238,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = ReduceLROnPlateau(optimizer, 'max', patience=3, min_lr=1e-6)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:48.984345Z","iopub.execute_input":"2024-10-04T06:06:48.985082Z","iopub.status.idle":"2024-10-04T06:06:48.992906Z","shell.execute_reply.started":"2024-10-04T06:06:48.985013Z","shell.execute_reply":"2024-10-04T06:06:48.991873Z"},"trusted":true},"execution_count":239,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:48.995119Z","iopub.execute_input":"2024-10-04T06:06:48.996536Z","iopub.status.idle":"2024-10-04T06:06:49.004860Z","shell.execute_reply.started":"2024-10-04T06:06:48.996449Z","shell.execute_reply":"2024-10-04T06:06:49.003800Z"},"trusted":true},"execution_count":240,"outputs":[{"execution_count":240,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:49.008176Z","iopub.execute_input":"2024-10-04T06:06:49.008527Z","iopub.status.idle":"2024-10-04T06:06:49.030973Z","shell.execute_reply.started":"2024-10-04T06:06:49.008487Z","shell.execute_reply":"2024-10-04T06:06:49.029763Z"},"trusted":true},"execution_count":241,"outputs":[]},{"cell_type":"code","source":"class CustomImageDataset(Dataset):\n    def __init__(self, image_dir, csv_file, transform=None):\n        self.image_dir = image_dir\n        self.transform = transform\n\n        # Read the CSV file\n        self.data_frame = pd.read_csv(csv_file)\n\n        # Ensure the CSV file has columns 'filename' and 'class'\n        assert 'image' in self.data_frame.columns\n        assert 'label' in self.data_frame.columns\n\n    def __len__(self):\n        return len(self.data_frame)\n\n    def __getitem__(self, idx):\n        # Get the filename and class label\n        img_name = self.data_frame.iloc[idx, 0]  # Get the filename from the CSV\n        class_label = self.data_frame.iloc[idx, 1] - 1  # Get the class label from the CSV\n\n        # Construct the path to the image based on its class label\n        class_folder = os.path.join(self.image_dir, str(class_label+1))  # Convert class label to string\n        img_path = os.path.join(class_folder, img_name)\n\n        # Load the image\n        image = Image.open(img_path).convert('RGB')\n\n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n\n        return image, class_label","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:49.032401Z","iopub.execute_input":"2024-10-04T06:06:49.032726Z","iopub.status.idle":"2024-10-04T06:06:49.044513Z","shell.execute_reply.started":"2024-10-04T06:06:49.032690Z","shell.execute_reply":"2024-10-04T06:06:49.043361Z"},"trusted":true},"execution_count":242,"outputs":[]},{"cell_type":"code","source":"image_directory = r\"/kaggle/input/raf-db-dataset/DATASET/train\"  # Directory containing class subfolders\ncsv_file_path = r\"/kaggle/input/raf-db-dataset/train_labels.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:49.048201Z","iopub.execute_input":"2024-10-04T06:06:49.048955Z","iopub.status.idle":"2024-10-04T06:06:49.055446Z","shell.execute_reply.started":"2024-10-04T06:06:49.048916Z","shell.execute_reply":"2024-10-04T06:06:49.054521Z"},"trusted":true},"execution_count":243,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomImageDataset(image_dir=image_directory, csv_file=csv_file_path, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:49.056967Z","iopub.execute_input":"2024-10-04T06:06:49.057325Z","iopub.status.idle":"2024-10-04T06:06:49.076882Z","shell.execute_reply.started":"2024-10-04T06:06:49.057285Z","shell.execute_reply":"2024-10-04T06:06:49.075932Z"},"trusted":true},"execution_count":244,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=8, num_workers=4, pin_memory=True, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:49.080268Z","iopub.execute_input":"2024-10-04T06:06:49.080601Z","iopub.status.idle":"2024-10-04T06:06:49.085888Z","shell.execute_reply.started":"2024-10-04T06:06:49.080566Z","shell.execute_reply":"2024-10-04T06:06:49.084709Z"},"trusted":true},"execution_count":245,"outputs":[]},{"cell_type":"code","source":"test_image_directory = r\"/kaggle/input/raf-db-dataset/DATASET/test\"\ntest_csv_file_path = r\"/kaggle/input/raf-db-dataset/test_labels.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:49.087082Z","iopub.execute_input":"2024-10-04T06:06:49.087390Z","iopub.status.idle":"2024-10-04T06:06:49.094905Z","shell.execute_reply.started":"2024-10-04T06:06:49.087353Z","shell.execute_reply":"2024-10-04T06:06:49.093934Z"},"trusted":true},"execution_count":246,"outputs":[]},{"cell_type":"code","source":"test_dataset = CustomImageDataset(test_image_directory, test_csv_file_path, transform)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:49.096156Z","iopub.execute_input":"2024-10-04T06:06:49.096479Z","iopub.status.idle":"2024-10-04T06:06:49.110689Z","shell.execute_reply.started":"2024-10-04T06:06:49.096443Z","shell.execute_reply":"2024-10-04T06:06:49.109723Z"},"trusted":true},"execution_count":247,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_dataset, batch_size=8, num_workers=4, pin_memory=True, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:06:49.112223Z","iopub.execute_input":"2024-10-04T06:06:49.112547Z","iopub.status.idle":"2024-10-04T06:06:49.117511Z","shell.execute_reply.started":"2024-10-04T06:06:49.112513Z","shell.execute_reply":"2024-10-04T06:06:49.116419Z"},"trusted":true},"execution_count":248,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(r\"/kaggle/working/best_model_epoch15_acc62.25554106910039.pth\"))","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:20:49.021802Z","iopub.execute_input":"2024-10-04T06:20:49.022246Z","iopub.status.idle":"2024-10-04T06:20:49.104326Z","shell.execute_reply.started":"2024-10-04T06:20:49.022196Z","shell.execute_reply":"2024-10-04T06:20:49.103132Z"},"trusted":true},"execution_count":251,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/449201146.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(r\"/kaggle/working/best_model_epoch15_acc62.25554106910039.pth\"))\n","output_type":"stream"},{"execution_count":251,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"def train_model(model, train_loader, test_loader, num_epochs):\n#     best_val_acc = 0\n#     early_stopping_counter = 0\n    best_acc = 0\n    for epoch in range(1, num_epochs+1):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # Training loop\n        for inputs, labels in tqdm(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        # Calculate training accuracy\n        train_acc = 100. * correct / total\n        \n        # Validation loop\n        model.eval()\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in tqdm(test_loader):\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n        \n        val_acc = 100. * val_correct / val_total\n        \n        print(f'Epoch {epoch}/{num_epochs}, Loss: {running_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n        \n        # Scheduler step\n        scheduler.step(val_acc)\n        \n        # Early stopping\n        if val_acc > best_acc:\n            best_acc = val_acc\n#             early_stopping_counter = 0\n            torch.save(model.state_dict(), f\"best_model_epoch{epoch}_acc{val_acc}.pth\")\n            print(\"Model Saved\")\n#         else:\n#             early_stopping_counter += 1\n#             if early_stopping_counter > patience:\n#                 print(\"Early stopping!\")\n#                 break\n","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:46:33.022641Z","iopub.execute_input":"2024-10-04T06:46:33.023736Z","iopub.status.idle":"2024-10-04T06:46:33.037149Z","shell.execute_reply.started":"2024-10-04T06:46:33.023679Z","shell.execute_reply":"2024-10-04T06:46:33.036102Z"},"trusted":true},"execution_count":261,"outputs":[]},{"cell_type":"code","source":"train_model(model, train_loader, test_loader, 20)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T06:46:33.855410Z","iopub.execute_input":"2024-10-04T06:46:33.856084Z","iopub.status.idle":"2024-10-04T06:59:23.841598Z","shell.execute_reply.started":"2024-10-04T06:46:33.856026Z","shell.execute_reply":"2024-10-04T06:59:23.840283Z"},"trusted":true},"execution_count":262,"outputs":[{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 46.82it/s]\n100%|██████████| 384/384 [00:07<00:00, 51.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 1328.5245, Train Acc: 70.00%, Val Acc: 62.68%\nModel Saved\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.84it/s]\n100%|██████████| 384/384 [00:06<00:00, 63.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20, Loss: 1344.6403, Train Acc: 69.53%, Val Acc: 63.43%\nModel Saved\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 46.84it/s]\n100%|██████████| 384/384 [00:06<00:00, 63.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20, Loss: 1323.4233, Train Acc: 70.08%, Val Acc: 61.28%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.34it/s]\n100%|██████████| 384/384 [00:06<00:00, 57.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20, Loss: 1341.3119, Train Acc: 69.46%, Val Acc: 61.99%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.14it/s]\n100%|██████████| 384/384 [00:06<00:00, 61.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20, Loss: 1337.2899, Train Acc: 69.96%, Val Acc: 61.60%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.78it/s]\n100%|██████████| 384/384 [00:06<00:00, 58.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/20, Loss: 1312.6264, Train Acc: 70.05%, Val Acc: 61.47%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 46.89it/s]\n100%|██████████| 384/384 [00:05<00:00, 64.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/20, Loss: 1334.8165, Train Acc: 69.68%, Val Acc: 62.94%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.23it/s]\n100%|██████████| 384/384 [00:06<00:00, 63.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/20, Loss: 1327.7697, Train Acc: 70.08%, Val Acc: 61.51%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.88it/s]\n100%|██████████| 384/384 [00:05<00:00, 64.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/20, Loss: 1333.1337, Train Acc: 69.73%, Val Acc: 61.77%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.74it/s]\n100%|██████████| 384/384 [00:06<00:00, 63.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/20, Loss: 1340.2644, Train Acc: 69.80%, Val Acc: 61.57%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.40it/s]\n100%|██████████| 384/384 [00:06<00:00, 57.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/20, Loss: 1338.5828, Train Acc: 69.33%, Val Acc: 61.96%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.80it/s]\n100%|██████████| 384/384 [00:06<00:00, 61.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/20, Loss: 1324.6903, Train Acc: 70.02%, Val Acc: 62.16%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:33<00:00, 46.34it/s]\n100%|██████████| 384/384 [00:06<00:00, 62.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/20, Loss: 1336.6331, Train Acc: 69.60%, Val Acc: 62.09%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.58it/s]\n100%|██████████| 384/384 [00:06<00:00, 60.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/20, Loss: 1341.3109, Train Acc: 69.85%, Val Acc: 62.74%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.52it/s]\n100%|██████████| 384/384 [00:05<00:00, 65.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/20, Loss: 1340.4976, Train Acc: 69.98%, Val Acc: 62.42%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:32<00:00, 47.91it/s]\n100%|██████████| 384/384 [00:06<00:00, 58.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/20, Loss: 1326.4616, Train Acc: 69.99%, Val Acc: 62.26%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:31<00:00, 48.47it/s]\n100%|██████████| 384/384 [00:06<00:00, 63.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/20, Loss: 1322.6271, Train Acc: 69.73%, Val Acc: 61.34%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:31<00:00, 48.54it/s]\n100%|██████████| 384/384 [00:05<00:00, 64.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/20, Loss: 1333.6229, Train Acc: 70.23%, Val Acc: 61.21%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:31<00:00, 48.53it/s]\n100%|██████████| 384/384 [00:06<00:00, 62.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/20, Loss: 1336.5736, Train Acc: 69.86%, Val Acc: 61.77%\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1534/1534 [00:31<00:00, 48.09it/s]\n100%|██████████| 384/384 [00:05<00:00, 64.90it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 20/20, Loss: 1318.6183, Train Acc: 70.08%, Val Acc: 61.77%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}